{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability \n",
    "\n",
    "Conditioning updates probabilities by incorporating (partial) information in the form of conditioning events. The conditioning operation is crucial for virtually every application of probability theory, but this notebook will focus on [Information Theory](https://en.wikipedia.org/wiki/Information_theory) and, more specifically, [Channel Coding (a.k.a. Error Detection and Correction)](https://en.wikipedia.org/wiki/Error_detection_and_correction). This is the study of *codes* for communicating over noisy channels, and has applications anywhere from WiFi to computer memory. Interestingly, the performance, and hence design, of such codes relies on probability theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Binary Symmetric Channel\n",
    "\n",
    "Assume the basic channel coding setup: Alice wants to transmit a \"message\" to Bob over over a *binary* channel (e.g. wire with on=1/off=0 states). The message could be anything (e.g. a number, word, or image), but it has to be *encoded* into a binary string for sending over the channel. Similarly, Bob has to *decode* the binary string he receives to convert it back to the original message. The convention for encodind and decoding is called the [code](https://en.wikipedia.org/wiki/Code).\n",
    "\n",
    "<a title=\"Benjamin D. Esham, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Binary_symmetric_channel_(en).svg\"><img width=\"512\" alt=\"Binary symmetric channel (en)\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Binary_symmetric_channel_%28en%29.svg/512px-Binary_symmetric_channel_%28en%29.svg.png\"></a>\n",
    "\n",
    "If the communication channel was perfect, the choice of code would be straightforward: we would use the most succinct (i.e. shortest) mapping of messages to binary strings. But the channel is \"noisy\" and can randomly change states with equal probability $p$ of switching from 0 to 1, and vice-versa; this classical setup is called the [binary symmetric channel](https://en.wikipedia.org/wiki/Binary_symmetric_channel). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Representation Code\n",
    "\n",
    "To fix ideas, assume the message is a digit in 0-9, and that all digits are equally likely. In other words, without receiving *any* message (no information) Bob would expect a digit in 0-9 with equal probability. Each message is then be *encoded* into a binary sequence of 0's and 1' according to *some* encoding function $E()$. The coded message is then transmitted over the channel, and Bob receives a (possibly corrupted) signal, which is then *decoded* into a message or an error ($err$), according to the decoding function $D()$. There are different ways to treat errors, depending on whether the channel allows for two-way communication (e.g. Bob can request a repeat if he gets an error), but to keep things simple we assume only one-way communication. \n",
    "\n",
    "To fix ideas, let our coding scheme map the 10 messages/digits, denoted $M_i,\\; i=0,\\ldots,9$  onto their classical binary representation, i.e. $E(0)=(0000)$, $E(1)=(0001)$, up to $E(9)=(1001)$. We use strings of length 4 because we need at least $\\log_2(10) = 3.3219$ bits to represent 10 numbers. This leaves us with $2^4-10=6$ unused strings, which the decoder maps to an error; the entire coding scheme is represented below:\n",
    "![Naive Code](./img/3/naive_code.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conditional Probabilities\n",
    "\n",
    "Note that there are two sources of randomnes, represented in red:\n",
    "1. the choice of message (from the perspective of Bob, who doesn't Alice's intentions), and \n",
    "2. the noisy transmission. \n",
    "\n",
    "The encoding and decoding are deterministic 1-to-1 functions (black arrows). Without information (i.e. received code), Bob knows that every message (digit) is equally likely to be chosen by Alice. Let's represent this by assigning events $M_0,\\ldots,M_9$ for each digit, where $P(M_i)=.1, \\;  i=0,\\ldots,9$.\n",
    "\n",
    "When Bob obtains information in the form of the received code $R_j$, he updates the message probabilities accordingly. He is now interested in $P(M_i|R_j)$, but this probability is not directly available. \n",
    "\n",
    "Nevertheless, we know $P(R_j|M_i)$ from the binary symmetric channel assumption, that the probability of an error in any bit is $p$ (independently of other errors). For example, for message $M_1=\\{1\\}$ with $E(M_1)=(0001)$ the probability of receiving code $(0100)$ ($R_4$) is $p^2 \\times (1-p)^{4-2}$, because the codes differ by 2 bits, so there were 2 errors and 4-2 correct bits (their location is irrelevant). More generally, we have: \n",
    "\n",
    "$$ P(R_j | M_i ) = p^{ d_{ham} ( E(M_i), R_j )  } \\times (1-p)^{ 4 - d_{ham} ( E(M_i), R_j ) } $$\n",
    "\n",
    "where $d_{ham}(\\cdot, \\cdot)$ is the [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance), i.e. the number of differring characters in two strings of the same length. Assuming the transmission error probability is $p=5\\%$, we can calculate *all* conditional probabilities of the form $P( R_j | M_i ),\\; j=0,\\ldots,9, \\; i = 0,\\ldots, 15$ in the $10 \\times 16$ matrix ```P_RgM```, where the row index represents the message, and the column index the received code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# List of all possible codes\n",
    "codes = []\n",
    "for j in np.arange(2**4):\n",
    "    codes.append( np.binary_repr(j, width = 4) )  \n",
    "    \n",
    "# Hamming distance function (from https://en.wikipedia.org/wiki/Hamming_distance)    \n",
    "def hamming(string1, string2): \n",
    "    dist_counter = 0\n",
    "    for n in range(len(string1)):\n",
    "        if string1[n] != string2[n]:\n",
    "            dist_counter += 1\n",
    "    return dist_counter\n",
    "\n",
    "# Distance matrix of all codes\n",
    "dist_mat = np.ones( [ len(codes), len(codes) ] )\n",
    "for i in range(len(codes)):\n",
    "    for j in range(len(codes)):\n",
    "        dist_mat[i,j] = hamming( codes[i], codes[j] )\n",
    "\n",
    "p = .05 # transmission error probability\n",
    "        \n",
    "# We use only the relevant rows (first 10) of the distance matrix \n",
    "# to caclulate the conditional probabilites \n",
    "P_RgM = ( p ** dist_mat[0:10,:] ) * ( (1-p) ** ( 4 - dist_mat[0:10,:] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows of ```P_RgM``` sum to 1, since each row represents a conditional probability measure, conditional on the (encoded) message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( P_RgM, axis = 1 ) # take row sums of matrix P_RgM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now answer questions such as: *If Bob received $R_4=(0100)\\rightarrow D(R_4)=4$, what is the probability that Alice actually sent $M_4 = \\{4\\}$ ?*, (This is an extension of Execise 2.12.(a) from [B&W](https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view)). \n",
    "![Bayes for coding](./img/3/binary_repr_code_bayes.svg)\n",
    "We are looking for $P(M_4|R_4)$ but we know $P(R_4|M_i)$ and $P(M_i)$ for all $i=0,\\ldots,9$, which is exactly the setup for *Bayes rule*:\n",
    "$$ P(M_i|R_j) = \\frac{ P(R_j|M_i) P(M_i) }{ P( R_j) } = \\frac{ P(M_i \\cap R_j) }{ \\sum_{i}  P( M_i \\cap R_j) } $$\n",
    "\n",
    "The main ingredient in both numerator and denominator are the *joint* probabilities $P(M_i \\cap R_j) = P(R_j|M_i) P(M_i)$, which are found by multiplying each element of the conditional probability matrix with the appropriate message probability. In our case, since all messages are equally likely, we can just multiply every element of ```P_RgM``` by 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_MR = P_RgM / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability $P(M_4|R_4) = \\frac{ P(M_4 \\cap R_4) } { \\sum_{i=0}^{9} P(M_i, R_4) } $ is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8552369077306735"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_MR[4,4] / sum( P_MR[:,4] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the probabilities of all messages given $R_4$, of which $P(M_4|R_4)$ is the most likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar( np.arange(10), P_MR[:,4] / sum( P_MR[:,4] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the conditional probability $P(M_3|R_4)$ is much smaller than $P(M_0|R_4)$, which means that if Bob receives $R_4$, Alice is more likely to have sent $M_0$ rather than $M_3$. That's because the conditional probabilities rely on Hamming distances of the codes, which depend on the encoding, and are not necessarily similar to the numerical distances of the messages/digits. The [tesseract](https://en.wikipedia.org/wiki/Tesseract) plot below illustrates the Hamming distances of all 4-digit binary codes, given by the minimum number of hops between two node/codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a title=\"en:User:Cburnett, CC BY-SA 3.0 &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Hamming_distance_4_bit_binary_example.svg\"><img width=\"256\" alt=\"Hamming distance 4 bit binary example\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Hamming_distance_4_bit_binary_example.svg/256px-Hamming_distance_4_bit_binary_example.svg.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code Performance\n",
    "\n",
    "For overall code performance, we look at the probability of decoding the correct message, as well as the probability of errors, either *detected* or *undetected*. In our example, the overall probability (across all messages) of a correct communication is \n",
    "$$ \\sum_{i=0}^{9} P( M_i \\cap R_i ) $$ \n",
    "This is given by the sum of the diagonal elements of the joint probability matrix, according to the following plot:\n",
    "![code_matrix](./img/3/code_matrix.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8145062499999999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( np.diag(P_MR) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a *detected* error, i.e. of received code mapping to $err$, is $ \\sum_{i=0}^{9} \\sum_{j=10}^{15} P( M_i \\cap R_j )$, which is the sum of the last 6 columns of ```P_MR```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.048525000000000006"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( P_MR[:,10:16], axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the probability of an *undetected* error is the sum of the off-diagonal elements of the $10\\times 10$ sub-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13696874999999975"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( P_MR[0:10,0:10], axis=None) - sum( np.diag(P_MR) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a 13.7% chance of getting an undetenced error, i.e. receiving a code that maps to a different message (but not $err$) than the intended one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modified Code\n",
    "Now that we know how to measure code performance, we can test different coding schemes. The basic idea is to employ a coding scheme that maps messages \"far\" from each other (in an appropriate sense, or distance metric) so as to minimize the probability of mixing them up. For example, let's change the coding scheme so that $9$ maps to $(1111)$, which is farther apart from the other codes in Hamming distance, and hence less likely to get mixed up. The joint probability matrix for this scheme is shown below (note the change in the last row's code):\n",
    "![Modified code](./img/3/mod_code.svg)\n",
    "and caclulated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_MR_v2 = ( p ** dist_mat[ [0,1,2,3,4,5,6,7,8,15], :] ) * \\\n",
    "    ( (1-p) ** ( 4 - dist_mat[ [0,1,2,3,4,5,6,7,8,15], :] ) ) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall probability of correct transmission is the same as before; this is straightforward to prove by symmetry. Therefore, the total probability of incorrent transmission is also the same, but the difference lies in the probability of detected vs silent errors. The probability of a detected error is now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057075"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( P_MR_v2[:,9:15] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the probability of a silent error is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12841875000000014"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - sum( np.diag( P_MR_v2[:,[0,1,2,3,4,5,6,7,8,15]] ) ) - np.sum( P_MR_v2[:,9:15] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is lower than the previous probability of 13.7%. The take-away is that mapping messages farther appart allows us to better detect errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error Correction\n",
    "\n",
    "Another common strategy is to automatically *correct* errors, by mapping mistakenly transmitted codes onto their likely original messages, called [forward error correction](https://en.wikipedia.org/wiki/Error_correction_code#Forward_error_correction). In the modified example above ($9 \\leftrightarrow (1111)$), assume we also decode $(1110)$ onto the digit $9$, since $(1110)$ is less likely to have come from other messages (except for $6 \\leftrightarrow (0110)$, but ignore this). The probability of correct transmission now includes an extra code, at the expense of inflating the probability of *silent* errors, as shown below:\n",
    "![Error Correction Code](./img/3/ecc.svg) \n",
    "The probability of correct transmission is now slightly higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8187931249999999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sum( np.diag( P_MR_v2[:,[0,1,2,3,4,5,6,7,8,15]] ) ) + P_MR_v2[9,14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the probability of a silent error is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13364437499999973"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(P_MR_v2[:,[0,1,2,3,4,5,6,7,8,14,15]], axis = None) - sum( np.diag( P_MR_v2[:,[0,1,2,3,4,5,6,7,8,15]] ) ) - P_MR_v2[9,14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fixed code length, there will be trade-offs between correct transmission, detected/corrected, and silent errors. But there is a way to improve *all* metrics by introducing *redundancy*, i.e. transmitting more bits that the absolute minimum necessary. Essentially, this \"expands the space\" of possible codes, so that you can map messages farther apart. \n",
    "\n",
    "But redundancy introduces concerns of *efficiency*: for example, you could transmit a digit with miniscule probability of error using a 1000-bit code, but that would use up too much channel capacity (time and resources for transmitting 1000 instead of 4 bits). There is actually a celebrated [theorem](https://en.wikipedia.org/wiki/Noisy-channel_coding_theorem) that provides the optimal trade-off between the accuracy and redundancy (code length) of a channel, but unfortunately without identifying the optimal code that achieves it. (Ironically, the theorem's proof uses *random* codes!!!) In fact,  even for \"simple\" setups such as the above, finding the optimal code is an [open problem](https://math.stackexchange.com/questions/4416045/how-to-find-the-maximal-minimal-distance-between-m-points-in-a-k-dimension-hammi).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "1. Assume the original setup above, i.e a binary symmetric channel ($p=.05\\%$) with binary representation for the 10 digits. To minimize errors, you decide to transmit every bit in the binary representation *3 times*, and take the majority bit as received; this is an example of a [repetition code](https://en.wikipedia.org/wiki/Repetition_code). E.g., $2$ would be mapped to $(0010)$, which would be transmitted as $((000)(000)(111)(000))$. At the other end, if Bob received $( (010) (\\cdots)(\\cdots)(\\cdots) )$, he would automatically correct $(010)$ to $0$, since 2/3 bits are $0$, etc. Find the probability of correct transmission for this scheme; this is a simple enough analytical caclulation that does not require Python. <br>\n",
    "(Note: This is an extension of Execise 2.12.(b) from [B&W](https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view). )\n",
    "2. Consider the [two-out-of-five code](https://en.wikipedia.org/wiki/Two-out-of-five_code), where each digit is mapped to a 5-bit string with two $1$'s and three $0$'s, as follows:\n",
    "| Digit | Binary Code |\n",
    "|:-----:|:------:|\n",
    "|   1   | 00011  |\n",
    "|   2   | 00101  |\n",
    "|   3   | 00110  |\n",
    "|   4   | 01001  |\n",
    "|   5   | 01010  |\n",
    "|   6   | 01100  |\n",
    "|   7   | 10001  |\n",
    "|   8   | 10010  |\n",
    "|   9   | 10100  |\n",
    "|   0   | 11000  |\n",
    "\n",
    "There are exactly $ { 5 \\choose 2 }= 10$ such codes, all equally apart in Hamming distance. Using a binary symmetric channel with $p=5\\%$, calculate the probability of correct transmission, detected error, and undetected error. <br>\n",
    "(Note there is no error correction in this scheme, any received code without two $1$'s is mapped to $err$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
