{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f47905",
   "metadata": {},
   "source": [
    "## Discrete Multivariate Distributions\n",
    "\n",
    "Multivariate distributions describe the probabilistic behavior of *multiple* quantities and their relationships. Every Statistical and Machine Learning method involves multiple random variables (RVs) and their distributions: joint,  conditionals, and margninals. In fact, the foundation of each method lies on how it models the dependence structure of the RVs, which is typically done in way that facilitates analysis and/or computation. In other words, different models make assumptions about how the variables are related to allow for efficient processing. Experienced Data Scientists understand the modelling assumptions and their limitations, allowing them to choose and improve appropriate methods for the problem at hand.\n",
    "\n",
    "In this notebook we build a [Naive Bayes Classifier (NBC)](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) for spam detection, one of the earliest and most widespread applications of [Statistical classification](https://en.wikipedia.org/wiki/Statistical_classification). Although the probability model for NBC is very simple, the method works extremely well for email filtering and other text classification. We will first look at the model details, and then we will implement the NBC in Python and compare its results with the built-in function in [scikit-learn](https://scikit-learn.org/stable/), Python's principal Machine Learning library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d08119",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification\n",
    "\n",
    "Classification is the problem of *predicting the class* of an object based on its other observed characteristics which are  probabilistically related to its class. We denote the class by the *categorical* (i.e., discrete finite) RV $Y$, called the *response* or *label*, and we denote the related variables by $X_1,\\ldots,X_p$, called the *predictors* of *features*. For out purposes we will assume the predictors are also discrete finite RVs. So, to summarize, the goal of NBC is to predict the (unknown) value of $Y$ based on the (observed) values of $X_1,\\ldots, X_p$. \n",
    "\n",
    "##### Maximum A Posteriori Prediction\n",
    "\n",
    "Assume we know the *joint* distribution of all #$(p+1)$ RVs $Y,X_1,\\ldots, X_p$ for starters. If we observed the values $x_1,\\ldots, x_p$ for $X_1,\\ldots, X_p$, our best description of $Y$ would be the *conditional* distribution:\n",
    "$$ P( Y = y | X_1 =x_1, \\ldots, X_p = x_p ),\\quad \\text{ for }  y \\in \\text{ range }(Y) $$\n",
    "which gives you the conditional probabilities of the possible values of $Y$. If we then had to *guess* which value the RV $Y$ takes, we would choose the *most likely* one, i.e. that which maximizes the conditional probability:\n",
    "$$ \\hat{y} =  \\arg \\max_y P( Y = y | X_1 =x_1, \\ldots, X_p = x_p ) $$\n",
    "This approach is called [Maximum a Posteriori (MAP)](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) estimation, from the name of the conditional distribution, which is called the [*posterior*](https://en.wikipedia.org/wiki/Posterior_probability) in this context. \n",
    "\n",
    "\n",
    "Furthermore, from Bayes theorem we get:\n",
    "$$ P(Y=y|X_1=x_1,\\ldots,X_p=x_p) = \\frac{  P(Y=y, X_1 = x_1, \\ldots, X_p = x_p ) } { P( X_1 = x_1, \\ldots, X_p = x_p ) } \\\\ = \\frac{  P( X_1 = x_1, \\ldots, X_p = x_p | Y=y ) P(Y=y) } { P( X_1 = x_1, \\ldots, X_p = x_p ) } $$\n",
    "Note that $y$ appears only in the numerator, so maximizing the posterior is equivalent to maximizing the numerator, which is essentially a re-expression of the *joint* distribution. This is the form of the maximization we will perform, in order to avoid unnecessary calculations. \n",
    "\n",
    "Also note that *if* the predictor RVs $X_1,\\ldots, X_p$ where *independent* of the response RV $Y$, then its conditional/posterior distribution would be the same as its *marginal* distribution:\n",
    "$$P( Y = y | X_1 =x_1, \\ldots, X_p = x_p ) = P( Y = y)$$\n",
    "Intuitively, independence implies that the information obtained from $X_1, \\ldots, X_p$ is irrelevant for predicting $Y$. When building a classifier, we would therefore want to use predictors that are as *dependent* as possible to the response. As an extreme example, if you had a predictor $X$ that was *perfectly* related to $Y$, i.e. there is a 1-to-1 relation between the values of $X$ and $Y$, then knowing the value of $X$ would tell you the value of $Y$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a62c77",
   "metadata": {},
   "source": [
    "##### Naive Bayes Assumption\n",
    "\n",
    "We have seen how Bayesian classification works, at least conceptually, but to actually *apply* it we need a workable form of the *joint* distribution of $Y,X_1,\\ldots, X_p$. Since all variables are finite, their joint distribution can be represented as a $(1+p)$-dimensional array. In Statistical/Machine Learning applications, the probability model is not provided, but has to be *estimated/trained* based on data. E.g., assuming all variables are *binary*, there are around $2^{(p+1)}$ probabilities to estimate in the joint distribution array. Because the number of parameters increases *exponentially* in the number of dimensions/predictors, good estimation quickly becomes infeasible (requires astronomical amounts of data); what is called the [curse of dimensionality](link here). \n",
    "\n",
    "To solve this problem, NBC makes a *naive* (not reallistic) simplifying assumption: it assumes features are *conditionally independent* given the class $Y$. In practice, this implies that:\n",
    "$$ P( X_1 = x_1, \\ldots, X_p = x_p | Y=y ) P(Y=y) \\\\\n",
    "= P(X_1 = x_1 | Y = y )  \\times \\cdots \\times P(X_p = x_p | Y = y ) \\times P( Y = y )  \\\\\n",
    "= \\left( \\prod_{i=1}^{p} P(X_i = x_i | Y = y ) \\right)  \\times P( Y = y ) $$\n",
    "I.e., the joint distribution can be expressed as a product of the $X$'s 1D conditionals times $Y$'s 1D marginal. By imposing this special structure we effectively reduce the dimensionality of the problem: instead of one $(p+1)$-dimensional distribution, we work with $(p+1)$ 1-dimensional distributions. E.g., if we assume RVs are binary, there are $2(p+1)$ probabilities to estimate, rather than $2^{(p+1)}-1$ for the general model. Now that we have know the main idea behind NBC, we look at a specific application for spam detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4e4ec",
   "metadata": {},
   "source": [
    "##### Spam Data  \n",
    "\n",
    "We will be working with an open data set of 5726 emails, of which 1368 are *spam* and the remaining 4358 *ham* (legitimate). These are a subset of the http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html\n",
    "We use the [pandas](...) library in Python for loading tabular/spreadsheet-like data, where each row represents and observation, and each column represents a field/variable. Column ```text``` contains the email text, and column ```spam``` indicates if the email is spam (1) or not (0); below is a preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c973b071",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/emails.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m emails \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/emails.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m emails\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/emails.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "emails = pd.read_csv(\"./data/emails.csv\",  usecols=[0, 1])\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5e5d8",
   "metadata": {},
   "source": [
    "The spam column is our response, and the text here will provide our predictors. For the latter, we construct predictor/feature variables that record for the presence or absence of select words in *all* emails. This is a common pre-processing step for text analysis called [tokenization], which is a crude way to represent text since it misses all structure and meaning (it is part of  [bag-of-words](link) models). \n",
    "\n",
    "For our tokenization, we use the ```feature_extraction.text.CountVectorizer``` function in ```scikit learn```. We remove infrequent (in <10 emails) and very frequent (in >30% of emails) words, since neither provides much information (e.g., if a word is in every email, it doesn't say much about it being spam). There are 6233 reamining words, each one representing a *binary* feature, where 1 means the word is present in the email and 0 means otherwise. Below is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "b2a89e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', 'aggregate', 'bright', 'contractors', 'easter', 'forever',\n",
       "       'indicating', 'loqo', 'official', 'projected', 'rush',\n",
       "       'subscribed', 'vieira'], dtype=object)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'english', binary = 'True', min_df=10, max_df = .30) # initialize vectorizer\n",
    "X = vectorizer.fit_transform( emails.text )   # apply vectorizer to emails text column; output is sparse binary matrix \n",
    "X_words = vectorizer.get_feature_names_out()  # extract word\n",
    "\n",
    "X_words[ range(0,X.shape[1],500) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "4253e223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6233"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "Y = emails.spam.array\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "527ce702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4358"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5726-sum(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5a0868d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "e1b588cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629888268156425"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "sum( y_pred == y_test ) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "c9d8f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1031   51]\n",
      " [   2  348]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97      1082\n",
      "           1       0.87      0.99      0.93       350\n",
      "\n",
      "    accuracy                           0.96      1432\n",
      "   macro avg       0.94      0.97      0.95      1432\n",
      "weighted avg       0.97      0.96      0.96      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred) )\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "2b84d825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629888268156425"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "P_X_y0 = ( np.sum( X_train[ y_train == 0 ] , 0) + 1 ) / ( sum( y_train == 0 ) + X_train.shape[1] )\n",
    "P_X_y1 = ( np.sum( X_train[ y_train == 1 ] , 0) + 1 ) / ( sum( y_train == 1 ) + X_train.shape[1] ) \n",
    "\n",
    "P_y0 = sum( y_train == 0 ) / len(y_train)\n",
    "P_y1 = sum( y_train == 1 ) / len(y_train)\n",
    "\n",
    "n_test = len(y_test)\n",
    "y_my_pred = np.zeros(len(y_test))\n",
    "\n",
    "for i in np.arange(0, len(y_test)) :\n",
    "    lP_y0X = np.sum( np.log( P_X_y0[ X_test[i].todense() == 1 ] ) ) + np.sum( np.log( 1 - P_X_y0[ X_test[i].todense() == 0 ] ) ) #+ np.log( P_y0 )\n",
    "    lP_y1X = np.sum( np.log( P_X_y1[ X_test[i].todense() == 1 ] ) ) + np.sum( np.log( 1 - P_X_y1[ X_test[i].todense() == 0 ] ) ) #+ np.log( P_y1 )\n",
    "    if lP_y1X > lP_y0X:\n",
    "        y_my_pred[i] = 1\n",
    "\n",
    "sum( y_pred == y_test ) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "0632ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1042   40]\n",
      " [  86  264]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94      1082\n",
      "           1       0.87      0.75      0.81       350\n",
      "\n",
      "    accuracy                           0.91      1432\n",
      "   macro avg       0.90      0.86      0.88      1432\n",
      "weighted avg       0.91      0.91      0.91      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_my_pred) )\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_my_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d3c48d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1018x6233 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 66870 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[ y_train > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "57344435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7629250116441546 0.23707498835584537 1.0\n"
     ]
    }
   ],
   "source": [
    "sum( y_train == 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adef0c91",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}