{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f47905",
   "metadata": {},
   "source": [
    "## Discrete Multivariate Distributions\n",
    "\n",
    "Multivariate distributions describe the probabilistic behavior of *multiple* quantities and their relationships. Every Statistical and Machine Learning method involves multiple random variables (RVs) and their distributions: joint,  conditionals, and margninals. In fact, the foundation of each method lies on how it models the dependence structure of the RVs, which is typically done in way that facilitates analysis and/or computation. In other words, different models make assumptions about how the variables are related to allow for efficient processing. Experienced Data Scientists understand the modelling assumptions and their limitations, allowing them to choose and improve appropriate methods for the problem at hand.\n",
    "\n",
    "In this notebook we build a [Naive Bayes Classifier (NBC)](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) for spam detection, one of the earliest and most widespread applications of [Statistical classification](https://en.wikipedia.org/wiki/Statistical_classification). Although the probability model for NBC is very simple, the method works extremely well for email filtering and other text classification. We will first look at the model details, and then we will implement the NBC in Python and compare its results with the built-in function in [scikit-learn](https://scikit-learn.org/stable/), Python's principal Machine Learning library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d08119",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification\n",
    "\n",
    "Classification is the problem of *predicting the class* of an object based on its other observed characteristics which are  probabilistically related to its class. We denote the class by the *categorical* (i.e., discrete finite) RV $Y$, called the *response* or *label*, and we denote the related variables by $X_1,\\ldots,X_p$, called the *predictors* of *features*. For out purposes we will assume the predictors are also discrete finite RVs. So, to summarize, the goal of NBC is to predict the (unknown) value of $Y$ based on the (observed) values of $X_1,\\ldots, X_p$. \n",
    "\n",
    "##### Maximum A Posteriori Classification\n",
    "\n",
    "Assume we know the *joint* distribution of all #$(p+1)$ RVs $Y,X_1,\\ldots, X_p$ for starters. If we observed the values $x_1,\\ldots, x_p$ for $X_1,\\ldots, X_p$, our best description of $Y$ would be the *conditional* distribution:\n",
    "$$ P( Y = y | X_1 =x_1, \\ldots, X_p = x_p ),\\quad \\text{ for }  y \\in \\text{ range }(Y) $$\n",
    "which gives you the conditional probabilities of the possible values of $Y$. If we then had to *guess* which value the RV $Y$ takes, we would choose the *most likely* one, i.e. that which maximizes the conditional probability:\n",
    "$$ \\hat{y} =  \\arg \\max_y P( Y = y | X_1 =x_1, \\ldots, X_p = x_p ) $$\n",
    "This approach is called [Maximum a Posteriori (MAP)](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) prediction, from the name of the conditional distribution, which is called the [*posterior*](https://en.wikipedia.org/wiki/Posterior_probability) in this context. \n",
    "\n",
    "\n",
    "Furthermore, from Bayes theorem we get:\n",
    "$$ P(Y=y|X_1=x_1,\\ldots,X_p=x_p) = \\frac{  P(Y=y, X_1 = x_1, \\ldots, X_p = x_p ) } { P( X_1 = x_1, \\ldots, X_p = x_p ) } \\\\ = \\frac{  P( X_1 = x_1, \\ldots, X_p = x_p | Y=y ) P(Y=y) } { P( X_1 = x_1, \\ldots, X_p = x_p ) } $$\n",
    "Note that $y$ appears only in the numerator, so maximizing the posterior is equivalent to maximizing the numerator, which is essentially a re-expression of the *joint* distribution. This is the form of the maximization we will perform, in order to avoid unnecessary calculations. \n",
    "\n",
    "Also note that *if* the predictor RVs $X_1,\\ldots, X_p$ where *independent* of the response RV $Y$, then its conditional/posterior distribution would be the same as its *marginal* distribution:\n",
    "$$P( Y = y | X_1 =x_1, \\ldots, X_p = x_p ) = P( Y = y)$$\n",
    "Intuitively, independence implies that the information obtained from $X_1, \\ldots, X_p$ is irrelevant for predicting $Y$. When building a classifier, we would therefore want to use predictors that are as *dependent* as possible to the response. As an extreme example, if you had a predictor $X$ that was *perfectly* related to $Y$, i.e. there is a 1-to-1 relation between the values of $X$ and $Y$, then knowing the value of $X$ would tell you the value of $Y$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a62c77",
   "metadata": {},
   "source": [
    "##### Naive Bayes Assumption\n",
    "\n",
    "We have seen how Bayesian classification works, at least conceptually, but to actually *apply* it we need a workable form of the *joint* distribution of $Y,X_1,\\ldots, X_p$. Since all variables are finite, their joint distribution can be represented as a $(1+p)$-dimensional array. In Statistical/Machine Learning applications, the probability model is not provided, but has to be *estimated/trained* based on data. E.g., assuming all variables are *binary*, there are around $2^{(p+1)}$ probabilities to estimate in the joint distribution array. Because the number of parameters increases *exponentially* in the number of dimensions/predictors, good estimation quickly becomes infeasible (requires astronomical amounts of data); what is called the [curse of dimensionality](link here). \n",
    "\n",
    "To solve this problem, NBC makes a *naive* (not reallistic) simplifying assumption: it assumes features are *conditionally independent* given the class $Y$. In practice, this implies that:\n",
    "$$ P( X_1 = x_1, \\ldots, X_p = x_p | Y=y ) P(Y=y) \\\\\n",
    "= P(X_1 = x_1 | Y = y )  \\times \\cdots \\times P(X_p = x_p | Y = y ) \\times P( Y = y )  \\\\\n",
    "= \\left( \\prod_{i=1}^{p} P(X_i = x_i | Y = y ) \\right)  \\times P( Y = y ) $$\n",
    "I.e., the joint distribution can be expressed as a product of the $X$'s 1D conditionals times $Y$'s 1D marginal. By imposing this special structure we effectively reduce the dimensionality of the problem: instead of one $(p+1)$-dimensional distribution, we work with $(p+1)$ 1-dimensional distributions. E.g., if we assume RVs are binary, there are $2(p+1)$ probabilities to estimate, rather than $2^{(p+1)}-1$ for the general model. Now that we have know the main idea behind NBC, we look at a specific application for spam detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4e4ec",
   "metadata": {},
   "source": [
    "##### Spam Data  \n",
    "\n",
    "We will be working with a subset of the [Enron-Spam dataset](http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html) consisting of 5726 emails, 1368 of which (23.9%) are *spam* and the remaining 4358 *ham* (legitimate). \n",
    "We use the [pandas](https://pandas.pydata.org/) library in Python to work with table/spreadsheet-like data, where each row represents an observation and each column a field/variable. Column ```text``` contains the email text, and column ```spam``` indicates if the email is spam (1) or not (0); below is a preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c973b071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "emails = pd.read_csv(\"./data/emails.csv\",  usecols=[0, 1])\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85b6033e",
   "metadata": {},
   "source": [
    "The spam column is the response, and the text will provide the features. We construct binary predictor/feature variables that record the presence or absence of various words in each email. Just looking at the presence/frequency of words is a crude way to analyze text, disregarding its structure, and is called 'bag-of-words'[https://en.wikipedia.org/wiki/Bag-of-words_model] modelling. \n",
    "\n",
    "To extract the word features, we use the function ```feature_extraction.text.CountVectorizer``` from ```scikit learn```.  \n",
    "The choice of word features relies on some heuristic rules: specifically, we remove English [stop-words](https://en.wikipedia.org/wiki/Stop_word) like articles or common verbs, as well as infrequent (<10 emails) or very frequent (>30% of emails) words, since they do not provide much information (e.g., if a word is in every/no email, it doesn't help distinguish spam). There are 6233 remaining words, each one represented by a *binary* feature where 1 means the word is present in the email and 0 otherwise. Below is the code to compute the features and a random sample of 10 feature words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a89e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['capture',\n",
       " 'reaching',\n",
       " 'aiesec',\n",
       " 'enclosed',\n",
       " 'bother',\n",
       " 'ordered',\n",
       " 'measures',\n",
       " 'narrow',\n",
       " 'spark',\n",
       " 'introduced']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'english', binary = 'True', min_df=10, max_df = .30) # initialize vectorizer\n",
    "X = vectorizer.fit_transform( emails.text )   # apply vectorizer to emails text column; output is sparse binary matrix \n",
    "X_words = vectorizer.get_feature_names_out()  # extract words\n",
    "Y = emails.spam                               # extract response\n",
    "\n",
    "import random; random.seed(1)\n",
    "random.sample( list(X_words), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01baa985",
   "metadata": {},
   "source": [
    "##### Probability Estimation\n",
    "\n",
    "We now estimate the joint distribution, which for NBC is given by the conditional probabilities of each word being present in a spam or non-spam email, i.e. $P( X_i=1 | Y=1 )$ and $P( X_i = 1 | Y=0)$ respectively. Notice that for any binary feature we have $ P( X_i=0 | Y=y ) = 1 - P( X_i=1 | Y=y)$ for any $y=0,1$ by the complement rule, i.e. just one probability value determines the conditional distribution. \n",
    "\n",
    "Let's focus on $ P( X_i=1 | Y=1 ) = \\frac{P( X_i=0, Y=1 ) }{P( Y=1 )}$: we could estimate this probability based on data by dividing the number of times word $i$ appears in spam emails (corresponding to $ \\{X_i=0\\} \\cap \\{Y=1\\}$) by the number of spam emails ($\\{Y=1\\}$) in our data. But this approach has a problem: if there are no spam emails with that word in our data, the estimated probability will be 0, which implies it is *impossible* to ever have a spam email with that word. To avoid such extremes we employ [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), adjusting the ratio to allow a small probability of observing a word in every type of email. The estimated conditional probabilities become:\n",
    "\n",
    "$$ \n",
    "\\hat{P}( X_i=1 | Y=1 ) = \\frac{ \\text{ # spam emails w/ word $i$}+1 }{ \\text{# spam emails}+2 } \\\\\n",
    "\\hat{P}( X_i=1 | Y=0 ) = \\frac{ \\text{ # ham emails w/ word $i$}+1 }{ \\text{# ham emails}+2 } \n",
    "$$\n",
    "(the *hat* notation $\\hat{P}$ indicates this is an estimate). This, combined with the estimated (marginal) probability of spam:\n",
    "$$ \\hat{P}(Y=1) = \\frac{ \\text{ # spam emails } } { \\text{ # emails } } $$\n",
    "is all we need for NBC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0868d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "P_X_Y1 = ( np.sum( X[ Y == 1 ] , axis = 0) + 1 ) / ( sum( Y == 1 ) + 2 ) # Estimate P( X_i=1 | Y=1 )\n",
    "P_X_Y0 = ( np.sum( X[ Y == 0 ] , axis = 0) + 1 ) / ( sum( Y == 0 ) + 2 ) # Estimate P( X_i=1 | Y=0 )\n",
    "P_Y1 = sum( Y == 1 ) / len(Y)  # Estimate P( Y=1 )\n",
    "P_Y0 = 1 - P_Y1\n",
    "\n",
    "# np.sum( X[ Y == 1 ] , axis = 0) returns a vector with word counts:\n",
    "# X[ Y==1 ] uses Boolean array indexing to extract spam (Y==1) rows\n",
    "# (https://numpy.org/doc/stable/user/basics.indexing.html)\n",
    "# and np.sum( . , axis = 0 ) sums along word feature columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb89d74",
   "metadata": {},
   "source": [
    "##### Classification\n",
    "\n",
    "With probabilities estimated, we now apply our classifier on the same data to evualuate its performance. Using only the word features ($X$'s), we compare the two conditional probabilities $P( Y=1 | X_1=x_1,\\ldots,X_p=x_p)$ vs $P( Y=0 | X_1=x_1,\\ldots,X_p=x_p)$ and choose the class ($Y$=0/ham or 1/spam) giving the highest one. We saw above that we only need to calculate the numerator (denominator is common), which by the Naive Bayes assumption is factorized as:\n",
    "$$ P( X_1 = x_1, \\ldots, X_p = x_p | Y=y ) P(Y=y) =  \\prod_{i=1}^{p} P(X_i = x_i | Y = y ) \\times P( Y = y ) $$\n",
    "If you tried to calculate this product in Python you would get 0 because it multiplies thousands of small probabilities. For numerical stability, we will compare the *logarithms* of the products which are the sums of their logs:\n",
    "$$ \\log  \\left( \\prod_{i=1}^{p} P(X_i = x_i | Y = y ) \\times P( Y = y ) \\right) = \\sum_{i=1}^{p} \\log ( P(X_i = x_i | Y = y ) ) + \\log(  P( Y = y ) ) $$\n",
    "The following code loops over every email, compares the log-probabilities for ham/spam, and selects the class with the highest one. Note the use of the complement rule $P(X_i=0|Y=y) = 1 - P(X_i=1|Y=y)$ for features with $X_i = 0$. At the end, it calculates the *accuracy*, i.e. the proportion of correctly classified emails, which is an impressive 96.75%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b84d825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9675165909884736"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_my_pred = np.zeros(len(Y))\n",
    "\n",
    "for i in np.arange(0, len(Y)) :\n",
    "    lP_Y0X = np.sum( np.log( P_X_Y0[ X[i].todense() == 1 ] ) ) + np.sum( np.log( 1 - P_X_Y0[ X[i].todense() == 0 ] ) ) + np.log( P_Y0 )\n",
    "    lP_Y1X = np.sum( np.log( P_X_Y1[ X[i].todense() == 1 ] ) ) + np.sum( np.log( 1 - P_X_Y1[ X[i].todense() == 0 ] ) ) + np.log( P_Y1 )\n",
    "    if lP_Y1X > lP_Y0X:\n",
    "        Y_my_pred[i] = 1\n",
    "\n",
    "sum( Y_my_pred == Y ) / len(Y)   # caclulate accuracy (% correct classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df172c13",
   "metadata": {},
   "source": [
    "We have effectively implemented the same procedure used in ```scikit-learn```, albeit less efficiently. For comparison, the following code gives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3c48d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1018x6233 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 66870 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b588cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9675165909884736"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()      # initiate Bernoulli (= binary feature) NBC \n",
    "clf.fit(X, Y)            # train model \n",
    "Y_pred = clf.predict(X)  # extract predictions\n",
    "sum( Y_pred == Y ) / len(Y) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d8f516",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(\u001b[43mY_test\u001b[49m, Y_pred) )\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(Y_test, Y_pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y_test, Y_pred) )\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "57344435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7629250116441546 0.23707498835584537 1.0\n"
     ]
    }
   ],
   "source": [
    "sum( y_train == 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35bcad",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "\n",
    "Before applying the classifier, we split our data into a [training and test](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) set; the training set is used to estimate the probabilities, and the test is used to evaluate the performance of the procedure. We do this to get an *objective* measure of performance on data that were not used to set up \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28bc6119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629888268156425"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.25, random_state=1234)\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "sum( Y_pred == Y_test ) / len(Y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y_test, Y_pred) )\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}