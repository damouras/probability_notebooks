{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f47905",
   "metadata": {},
   "source": [
    "## Discrete Multivariate Distributions\n",
    "\n",
    "Multivariate distributions describe the probabilistic behavior of *multiple* quantities and their relationships. Every Statistical and Machine Learning method involves multiple random variables (RVs) and their distributions: joint,  conditionals, and margninals. In fact, the foundation of each method lies on how it models the dependence structure of the RVs, which is typically done in way that facilitates analysis and/or computation. In other words, different models make assumptions about how the variables are related to allow for efficient processing. Experienced Data Scientists understand the modelling assumptions and their limitations, allowing them to choose and improve appropriate methods for the problem at hand.\n",
    "\n",
    "In this notebook we build a [Naive Bayes Classifier (NBC)](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) for spam detection, one of the earliest and most widespread applications of [Statistical classification](https://en.wikipedia.org/wiki/Statistical_classification). Although the probability model for NBC is very simple, the method works extremely well for email filtering and other text classification. We will first look at the model details, and then we will implement the NBC in Python and compare its results with the built-in function in [scikit-learn](https://scikit-learn.org/stable/), Python's principal Machine Learning library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d08119",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification\n",
    "\n",
    "Classification is the problem of *predicting the class* of an object based on its other observed characteristics which are  probabilistically related to its class. We denote the class by the *categorical* (i.e., discrete finite) RV $Y$, called the *response* or *label*, and we denote the related variables by $X_1,\\ldots,X_p$, called the *predictors* of *features*. For out purposes we will assume the predictors are also discrete finite RVs. So, to summarize, the goal of NBC is to predict the (unknown) value of $Y$ based on the (observed) values of $X_1,\\ldots, X_p$. \n",
    "\n",
    "##### Maximum A Posteriori Prediction\n",
    "\n",
    "Assume we know the *joint* distribution of all #$(p+1)$ RVs $Y,X_1,\\ldots, X_p$ for starters. If we observed the values $x_1,\\ldots, x_p$ for $X_1,\\ldots, X_p$, our best description of $Y$ would be the *conditional* distribution:\n",
    "$$ P( Y = y | X_1 =x_1, \\ldots, X_p = x_p ),\\quad \\text{ for }  y \\in \\text{ range }(Y) $$\n",
    "which gives you the conditional probabilities of the possible values of $Y$. If we then had to *guess* which value the RV $Y$ takes, we would choose the *most likely* one, i.e. that which maximizes the conditional probability:\n",
    "$$ \\hat{y} =  \\arg \\max_y P( Y = y | X_1 =x_1, \\ldots, X_p = x_p ) $$\n",
    "This approach is called [Maximum a Posteriori (MAP)](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) estimation, from the name of the conditional distribution, which is called the [*posterior*](https://en.wikipedia.org/wiki/Posterior_probability) in this context. \n",
    "\n",
    "\n",
    "Furthermore, from Bayes theorem we get:\n",
    "$$ P(Y=y|X_1=x_1,\\ldots,X_p=x_p) = \\frac{  P(Y=y, X_1 = x_1, \\ldots, X_p = x_p ) } { P( X_1 = x_1, \\ldots, X_p = x_p ) } \\\\ = \\frac{  P( X_1 = x_1, \\ldots, X_p = x_p | Y=y ) P(Y=y) } { P( X_1 = x_1, \\ldots, X_p = x_p ) } $$\n",
    "Note that $y$ appears only in the numerator, so maximizing the posterior is equivalent to maximizing the numerator, which is essentially a re-expression of the *joint* distribution. This is the form of the maximization we will perform, in order to avoid unnecessary calculations. \n",
    "\n",
    "Also note that *if* the predictor RVs $X_1,\\ldots, X_p$ where *independent* of the response RV $Y$, then its conditional/posterior distribution would be the same as its *marginal* distribution:\n",
    "$$P( Y = y | X_1 =x_1, \\ldots, X_p = x_p ) = P( Y = y)$$\n",
    "Intuitively, independence implies that the information obtained from $X_1, \\ldots, X_p$ is irrelevant for predicting $Y$. When building a classifier, we would therefore want to use predictors that are as *dependent* as possible to the response. As an extreme example, if you had a predictor $X$ that was *perfectly* related to $Y$, i.e. there is a 1-to-1 relation between the values of $X$ and $Y$, then knowing the value of $X$ would tell you the value of $Y$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a62c77",
   "metadata": {},
   "source": [
    "##### Naive Bayes Assumption\n",
    "\n",
    "We have seen how Bayesian classification works, at least conceptually, but to actually *apply* it we need a workable form of the *joint* distribution of $Y,X_1,\\ldots, X_p$. Since all variables are finite, their joint distribution can be represented as a $(1+p)$-dimensional array. In Statistical/Machine Learning applications, the probability model is not provided, but has to be *estimated/trained* based on data. E.g., assuming all variables are *binary*, there are around $2^{(p+1)}$ probabilities to estimate in the joint distribution array. Because the number of parameters increases *exponentially* in the number of dimensions/predictors, good estimation quickly becomes infeasible (requires astronomical amounts of data); what is called the [curse of dimensionality](link here). \n",
    "\n",
    "To solve this problem, NBC makes a *naive* (not reallistic) simplifying assumption: it assumes features are *conditionally independent* given the class $Y$. In practice, this implies that:\n",
    "$$ P( X_1 = x_1, \\ldots, X_p = x_p | Y=y ) P(Y=y) \\\\\n",
    "= P(X_1 = x_1 | Y = y )  \\times \\cdots \\times P(X_p = x_p | Y = y ) \\times P( Y = y )  \\\\\n",
    "= \\left( \\prod_{i=1}^{p} P(X_i = x_i | Y = y ) \\right)  \\times P( Y = y ) $$\n",
    "I.e., the joint distribution can be expressed as a product of the $X$'s 1D conditionals times $Y$'s 1D marginal. By imposing this special structure we effectively reduce the dimensionality of the problem: instead of one $(p+1)$-dimensional distribution, we work with $(p+1)$ 1-dimensional distributions. E.g., if we assume RVs are binary, there are $2(p+1)$ probabilities to estimate, rather than $2^{(p+1)}-1$ for the general model. Now that we have know the main idea behind NBC, we look at a specific application for spam detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4e4ec",
   "metadata": {},
   "source": [
    "##### Spam Data  \n",
    "\n",
    "We will be working with an open data set of 5726 emails, of which 1368 are *spam* and the remaining 4358 *ham* (legitimate). These are a subset of the http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html\n",
    "We use the [pandas](...) library in Python for loading tabular/spreadsheet-like data, where each row represents and observation, and each column represents a field/variable. Column ```text``` contains the email text, and column ```spam``` indicates if the email is spam (1) or not (0); below is a preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c973b071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "emails = pd.read_csv(\"./data/emails.csv\",  usecols=[0, 1])\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5e5d8",
   "metadata": {},
   "source": [
    "The spam column is our response, and the text here will provide our predictors. For the latter, we construct predictor/feature variables that record for the presence or absence of select words in *all* emails. This is a common pre-processing step for text analysis called [tokenization], which is a crude way to represent text since it misses all structure and meaning (it is part of  [bag-of-words](link) models). \n",
    "\n",
    "For our tokenization, we use the ```feature_extraction.text.CountVectorizer``` function in ```scikit learn```. We remove infrequent (in <10 emails) and very frequent (in >30% of emails) words, since neither provides much information (e.g., if a word is in every email, it doesn't say much about it being spam). There are 6233 reamining words, each one representing a *binary* feature, where 1 means the word is present in the email and 0 means otherwise. Below is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a89e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', 'aggregate', 'bright', 'contractors', 'easter', 'forever',\n",
       "       'indicating', 'loqo', 'official', 'projected', 'rush',\n",
       "       'subscribed', 'vieira'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'english', binary = 'True', min_df=10, max_df = .30) # initialize vectorizer\n",
    "X = vectorizer.fit_transform( emails.text )   # apply vectorizer to emails text column; output is sparse binary matrix \n",
    "X_words = vectorizer.get_feature_names_out()  # extract word\n",
    "\n",
    "X_words[ range(0,X.shape[1],500) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4253e223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6233"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "Y = emails.spam.array\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527ce702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4358"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5726-sum(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0868d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split( X, \u001b[43my\u001b[49m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "e1b588cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629888268156425"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "sum( y_pred == y_test ) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "c9d8f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1031   51]\n",
      " [   2  348]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97      1082\n",
      "           1       0.87      0.99      0.93       350\n",
      "\n",
      "    accuracy                           0.96      1432\n",
      "   macro avg       0.94      0.97      0.95      1432\n",
      "weighted avg       0.97      0.96      0.96      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred) )\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "2b84d825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629888268156425"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "P_X_y0 = ( np.sum( X_train[ y_train == 0 ] , 0) + 1 ) / ( sum( y_train == 0 ) + X_train.shape[1] )\n",
    "P_X_y1 = ( np.sum( X_train[ y_train == 1 ] , 0) + 1 ) / ( sum( y_train == 1 ) + X_train.shape[1] ) \n",
    "\n",
    "P_y0 = sum( y_train == 0 ) / len(y_train)\n",
    "P_y1 = sum( y_train == 1 ) / len(y_train)\n",
    "\n",
    "n_test = len(y_test)\n",
    "y_my_pred = np.zeros(len(y_test))\n",
    "\n",
    "for i in np.arange(0, len(y_test)) :\n",
    "    lP_y0X = np.sum( np.log( P_X_y0[ X_test[i].todense() == 1 ] ) ) + np.sum( np.log( 1 - P_X_y0[ X_test[i].todense() == 0 ] ) ) #+ np.log( P_y0 )\n",
    "    lP_y1X = np.sum( np.log( P_X_y1[ X_test[i].todense() == 1 ] ) ) + np.sum( np.log( 1 - P_X_y1[ X_test[i].todense() == 0 ] ) ) #+ np.log( P_y1 )\n",
    "    if lP_y1X > lP_y0X:\n",
    "        y_my_pred[i] = 1\n",
    "\n",
    "sum( y_pred == y_test ) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "0632ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1042   40]\n",
      " [  86  264]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94      1082\n",
      "           1       0.87      0.75      0.81       350\n",
      "\n",
      "    accuracy                           0.91      1432\n",
      "   macro avg       0.90      0.86      0.88      1432\n",
      "weighted avg       0.91      0.91      0.91      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_my_pred) )\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_my_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d3c48d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1018x6233 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 66870 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[ y_train > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "57344435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7629250116441546 0.23707498835584537 1.0\n"
     ]
    }
   ],
   "source": [
    "sum( y_train == 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adef0c91",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}